{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class itemDataset(Dataset):\n",
    "    def __init__(self,file_name,mode='train',transform=None):\n",
    "\n",
    "        if(mode=='test'):\n",
    "            self.token = {}\n",
    "            for name in ['nodes','edges','tokens']:\n",
    "                self.token[name] =  {}\n",
    "                with open('./token/{0}'.format(name)) as f:\n",
    "                    for i,line in enumerate(f):\n",
    "                        self.token[name][line.strip()] = i\n",
    "        elif(mode=='train'):\n",
    "            self.token = {}\n",
    "            for name in ['nodes','edges','tokens']:\n",
    "                self.token[name] =  {}\n",
    "                self.token[name]['pad'] = 0\n",
    "                \n",
    "        self.read_json(file_name)\n",
    "        \n",
    "        if(mode=='train'):\n",
    "            for name in ['nodes','edges','tokens']:\n",
    "                with open('./token/{0}'.format(name),'w') as f:\n",
    "                    for name in self.token[name]:\n",
    "                        f.write(\"{0}\\n\".format(name))\n",
    "        self.transform = transform\n",
    "\n",
    "    def read_json(self,file_name):\n",
    "        def type2id(data,dtype):\n",
    "            for name in data:\n",
    "                try:\n",
    "                    return self.token[dtype][name]\n",
    "                except:\n",
    "                    self.token[dtype][name] = len(self.token[dtype])\n",
    "                    return self.token[dtype][name]\n",
    "        \n",
    "        def word2id(data):\n",
    "            ans = []\n",
    "            for word in data:\n",
    "                word = word.lower()\n",
    "                try:\n",
    "                    ans.append(self.token['tokens'][word])\n",
    "                except:\n",
    "                    self.token['tokens'][word] = len(self.token['tokens'])\n",
    "                    ans.append(self.token['tokens'][word])\n",
    "            return ans\n",
    "\n",
    "        self.data = []\n",
    "        self.sent = []\n",
    "        for i,line in enumerate(open(file_name)):\n",
    "            temp = json.loads(line)\n",
    "            for j in range(len(temp['nodes'])):\n",
    "                temp['nodes'][j] = [temp['nodes'][j][0],type2id(temp['nodes'][j][1],'nodes')]\n",
    "            for j in range(len(temp['edges'])):\n",
    "                temp['edges'][j] = [temp['edges'][j][0],temp['edges'][j][1],type2id(temp['edges'][j][2],'edges')]\n",
    "            \n",
    "            for j in range(len(temp['edges'])):\n",
    "                self.data.append( temp['edges'][j] )\n",
    "                self.data[-1].append(i)\n",
    "            self.sent.append({'tokens':word2id(temp['tokens']),'nodes':temp['nodes']})\n",
    "            \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {}\n",
    "        sample['edge'] = [self.data[idx][0],self.data[idx][1]]\n",
    "        sample['sent'] = self.sent[self.data[idx][3]]['tokens']\n",
    "        sample['nodes'] = self.sent[self.data[idx][3]]['nodes']\n",
    "        \n",
    "        sample['label'] = self.data[idx][2]\n",
    "\n",
    "        if(transforms):\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "\n",
    "class ToTensor(object):\n",
    "    def __call__(self,sample):\n",
    "        sample['sent'] = torch.tensor(sample['sent'],dtype=torch.long)\n",
    "        sample['sent_len'] = len(sample['sent'])\n",
    "        sample['label'] = torch.tensor(sample['label'],dtype=torch.long)\n",
    "        return sample.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = itemDataset('./train.json',transform=transforms.Compose([ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(sample):\n",
    "    data = {}\n",
    "    \n",
    "    for name in ['sent_len','label']:\n",
    "        data[name] = torch.tensor([_[name] for _ in sample],dtype=torch.long)\n",
    "        \n",
    "    batch_size,sent_len = len(data['sent_len']),data['sent_len'].max().item()\n",
    "    \n",
    "    data['sent'] = torch.stack([ torch.cat([ _['sent'],torch.zeros(sent_len-_['sent_len'],dtype=torch.long) ] ) for _ in sample])\n",
    "    \n",
    "    \n",
    "    data['node'] = torch.zeros(batch_size,sent_len,dtype=torch.long)\n",
    "    \n",
    "    for i in range(len(sample)):\n",
    "        for line in sample[i]['nodes']:\n",
    "            for num in range(line[0][0],line[0][1]):\n",
    "                data['node'][i][num] = line[1]\n",
    "    \n",
    "    data['edge'] = torch.zeros(batch_size,sent_len,dtype=torch.long) \n",
    "    for i in range(len(sample)):\n",
    "        for j,line in enumerate(sample[i]['edge']):\n",
    "            for num in range(line[0],line[1]):\n",
    "                data['edge'][i][num] = j+1\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = collate_fn([traindata[0],traindata[30]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodes 16\n",
      "edges 4\n",
      "tokens 5096\n"
     ]
    }
   ],
   "source": [
    "for name in traindata.token:\n",
    "    print(name,len(traindata.token[name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Temp:\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Temp()\n",
    "args.hidden_dim = 128\n",
    "\n",
    "args.num_layer = 1\n",
    "args.bidirectional = True\n",
    "args.batch_first = True\n",
    "args.dropout=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sent_len': tensor([47, 58]),\n",
       " 'label': tensor([1, 2]),\n",
       " 'sent': tensor([[ 1,  2,  3,  4,  5,  6,  7,  4,  8,  4,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "          17,  1, 18,  4, 19, 20, 21, 11, 22, 13, 23, 24, 25, 26, 27, 28, 29, 30,\n",
       "          21, 11, 31, 13, 32, 33, 28, 29, 34, 35, 36,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0],\n",
       "         [37, 38, 39, 30, 40,  4, 41, 42, 43, 44, 21, 45, 46, 47, 48,  4, 49,  4,\n",
       "          50, 30, 51, 52, 53, 54, 55, 56, 57,  4, 58,  4, 59, 50,  4, 60, 54, 61,\n",
       "          62, 63,  4, 64,  4, 59, 50,  4, 65, 54, 30, 66, 67, 68,  4, 69,  4, 59,\n",
       "          50,  4, 70, 36]]),\n",
       " 'node': tensor([[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 0, 0, 4, 4, 4, 4, 0, 0, 0,\n",
       "          0, 3, 3, 3, 0, 1, 1, 0, 0, 0, 2, 0, 0, 3, 3, 3, 1, 0, 0, 2, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 0, 4, 4, 0, 3, 0, 6, 6, 6, 6, 6, 0,\n",
       "          4, 4, 4, 0, 3, 0, 6, 6, 6, 6, 0, 4, 4, 4, 0, 3, 0, 6, 6, 6, 6, 0, 0, 4,\n",
       "          4, 4, 0, 3, 0, 6, 6, 6, 6, 0]]),\n",
       " 'edge': tensor([[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self,token,args):\n",
    "        super(RNN,self).__init__()\n",
    "        \n",
    "        self.word_emb = nn.Embedding(len(token['tokens']),args.hidden_dim,padding_idx=0)\n",
    "        self.ner_emb = nn.Embedding(len(token['nodes']),args.hidden_dim,padding_idx=0)\n",
    "        self.edge_emb = nn.Embedding(3,args.hidden_dim,padding_idx=0)\n",
    "    \n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=args.hidden_dim,\n",
    "            hidden_size=args.hidden_dim,\n",
    "            num_layers=args.num_layer,\n",
    "            batch_first=args.batch_first,\n",
    "            dropout=args.dropout,\n",
    "            bidirectional=args.bidirectional\n",
    "        )\n",
    "        \n",
    "        self.hidden_size = args.hidden_dim\n",
    "        self.num_layer = args.num_layer\n",
    "        self.batch_first = args.batch_first\n",
    "        self.dropout = args.dropout\n",
    "        self.bidirectional = args.bidirectional\n",
    "        \n",
    "        self.dense_1 = nn.Linear(2*args.hidden_dim,32)\n",
    "        self.act_1 = nn.ReLU()\n",
    "        self.dense_2 = nn.Linear(32,len(token['edges']))\n",
    "        \n",
    "        \n",
    "    def forward(self,data,data_len,data_ner,data_point):\n",
    "        def pack(seq,seq_length):\n",
    "            sorted_seq_lengths, indices = torch.sort(seq_length, descending=True)\n",
    "            _, desorted_indices = torch.sort(indices, descending=False)\n",
    "\n",
    "            if self.batch_first:\n",
    "                seq = seq[indices]\n",
    "            else:\n",
    "                seq = seq[:, indices]\n",
    "            packed_inputs = nn.utils.rnn.pack_padded_sequence(seq,\n",
    "                                                            sorted_seq_lengths.cpu().numpy(),\n",
    "                                                            batch_first=self.batch_first)\n",
    "\n",
    "            return packed_inputs,desorted_indices\n",
    "\n",
    "        def unpack(res, state,desorted_indices):\n",
    "            padded_res,_ = nn.utils.rnn.pad_packed_sequence(res, batch_first=self.batch_first)\n",
    "\n",
    "            state = [state[i][:,desorted_indices] for i in range(len(state)) ] \n",
    "\n",
    "            if(self.batch_first):\n",
    "                desorted_res = padded_res[desorted_indices]\n",
    "            else:\n",
    "                desorted_res = padded_res[:, desorted_indices]\n",
    "\n",
    "            return desorted_res,state\n",
    "\n",
    "        def feat_extract(output,length,mask):\n",
    "            \"\"\"\n",
    "            answer_output: batch*sentence*feat_len\n",
    "            query_output:  batch*sentence*feat_len\n",
    "            for simple rnn, we just take the output from \n",
    "            \"\"\"\n",
    "            if( self.batch_first == False ):\n",
    "                output = output.transpose(0,1) \n",
    "\n",
    "            output = [torch.cat([ output[i][ length[i]-1 ][:self.hidden_size] , \n",
    "                                        output[i][0][self.hidden_size:]] , dim=-1 ) for i in range(length.shape[0])]\n",
    "            output = torch.stack(output,dim=0)\n",
    "\n",
    "            return output\n",
    "        #first check for the mask ans the embedding\n",
    "        mask =  data.eq(0)\n",
    "\n",
    "        word = self.word_emb(data)\n",
    "        word = word + self.ner_emb(data_ner)\n",
    "        word = word + self.edge_emb(data_point)\n",
    "        \n",
    "        #query part\n",
    "        packed_inputs,desorted_indices = pack(word,data_len)\n",
    "        res, state = self.rnn(packed_inputs)\n",
    "        query_res,_ = unpack(res, state,desorted_indices)\n",
    "\n",
    "        #extract the representation of the sentence\n",
    "        query_result = feat_extract(query_res,data_len.int(),mask)\n",
    "\n",
    "        output = self.dense_1(query_result)\n",
    "        output = self.act_1(output)\n",
    "        output = self.dense_2(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(traindata.token,args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(data['sent'],data['sent_len'],data['node'],data['edge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(output,data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
